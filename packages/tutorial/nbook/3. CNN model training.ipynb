{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f990c261-e52b-4b32-aa3d-003be634d651",
   "metadata": {},
   "source": [
    "# CNN training example\n",
    "\n",
    "This notebook illustrates how to use EDIT pipeline to train a simple CNN model on the ERA5 lowres dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629b9551-8ff2-46d2-bc70-db1a84a6192e",
   "metadata": {},
   "source": [
    "Make sure to set the `ERA5LOWRES` environment variable to make the ERA5 low-resolution archive foundable on your system.\n",
    "Modify the following cell as follows:\n",
    "\n",
    "- for NCI\n",
    "\n",
    "```\n",
    "%env ERA5LOWRES=/g/data/wb00/NCI-Weatherbench/5.625deg\n",
    "```\n",
    "\n",
    "- for NIWA\n",
    "\n",
    "```\n",
    "%env ERA5LOWRES=/nesi/nobackup/niwa00004/riom/weatherbench/5.625deg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613f5e9f-0378-4c74-93ee-c49083bee302",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env ERA5LOWRES=/nesi/nobackup/niwa00004/riom/weatherbench/5.625deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd1e8b6-acc3-4074-92db-536c0e8ea112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import scores\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning import Trainer, LightningModule\n",
    "from lightning.pytorch.callbacks import RichProgressBar\n",
    "\n",
    "import edit.data\n",
    "import edit.tutorial\n",
    "import edit.pipeline\n",
    "import edit.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cd2afc-fdc1-418c-a4a0-59d8cd080de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/validation/test split dates\n",
    "train_start = \"2015-01-01T00\"\n",
    "train_end = \"2015-01-12T00\"\n",
    "val_start = \"2016-01-01T00\"\n",
    "val_end = \"2016-01-12T00\"\n",
    "test_start = \"2017-01-01T00\"\n",
    "test_end = \"2017-01-12T00\"\n",
    "\n",
    "# number of samples to estimate mean & standard deviation of fields\n",
    "n_samples = 200\n",
    "# folder to save estimated mean & standard deviation of fields\n",
    "stats_folder = \"cnn_training/stats\"\n",
    "\n",
    "# folders used to cache dataset processed by the pipeline\n",
    "cache_folder = \"cnn_training/cache\"\n",
    "\n",
    "# data loader parameters\n",
    "batch_size = 1\n",
    "n_workers = 2\n",
    "\n",
    "# trainer parameters\n",
    "default_root_dir = \"cnn_training\"\n",
    "max_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20df4768-94e6-435c-8e4b-d7ebb2f86e52",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09783db6-2e4e-4e57-8a66-6121a781e737",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_preparation = edit.pipeline.Pipeline(\n",
    "    edit.data.archive.era5lowres([\"u\", \"v\", \"geopotential\", \"vorticity\"]),\n",
    "    edit.pipeline.operations.xarray.Sort(\n",
    "        [\"msl\", \"10u\", \"10v\", \"2t\", \"geopotential\", \"vorticity\"]\n",
    "    ),\n",
    "    edit.data.transforms.coordinates.standard_longitude(type=\"0-360\"),\n",
    "    edit.pipeline.operations.xarray.reshape.CoordinateFlatten(\"level\"),\n",
    "    # retrieve previous/next samples, dt = 1H\n",
    "    edit.pipeline.modifications.TemporalRetrieval(\n",
    "        concat=True, samples=((-1, 1), (1, 1, 1))\n",
    "    ),\n",
    "    edit.pipeline.operations.xarray.conversion.ToNumpy(),\n",
    "    edit.pipeline.operations.numpy.reshape.Rearrange(\"c t h w -> t c h w\"),\n",
    "    edit.pipeline.operations.numpy.reshape.Squish(axis=0),\n",
    ")\n",
    "data_preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4c4ff2-78ac-4ab8-bce2-ffbcadb202e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data_preparation[train_start]\n",
    "print(len(sample))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c4baca-afe0-4450-9371-7aa57fb9e39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = edit.pipeline.iterators.DateRange(train_start, train_end, interval=\"1h\")\n",
    "train_split = train_split.randomise(seed=42)\n",
    "val_split = edit.pipeline.iterators.DateRange(val_start, val_end, interval=\"1h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927ca606-6031-44b4-a646-4b88528f3f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8465ac6f-4c01-4f6e-ae71-bf399d7fb764",
   "metadata": {},
   "source": [
    "We use then use precomputed approximate mean and standard deviation using only few random samples, to rescale the input/output data to a reasonable range for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b69b1c-780f-4890-bf00-8409026205bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_folder = Path(stats_folder)\n",
    "stats_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "mean_path = stats_folder / \"mean.npy\"\n",
    "std_path = stats_folder / \"std.npy\"\n",
    "\n",
    "# compute mean/std only if files are missing, to save time\n",
    "if not mean_path.is_file() or not std_path.is_file():\n",
    "    samples = np.stack([data_preparation[train_split[i]][0] for i in range(n_samples)])\n",
    "    mean_approx = np.mean(samples, axis=0)\n",
    "    std_approx = np.std(samples, axis=0)\n",
    "    np.save(mean_path, mean_approx)\n",
    "    np.save(std_path, std_approx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebf6ab9-1b81-42f0-b7f7-073491114bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "normaliser = edit.pipeline.operations.numpy.normalisation.Deviation(\n",
    "    mean=mean_path, deviation=std_path\n",
    ")\n",
    "caching_step = edit.pipeline.modifications.Cache(\n",
    "    cache_folder, pattern_kwargs={'extension': 'npy'}\n",
    ")\n",
    "data_preparation_normed = edit.pipeline.Pipeline(\n",
    "    data_preparation, normaliser, caching_step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d6e689-f5c7-4b6b-a88e-d7d12c8997fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preparation_normed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96d615b-b726-4568-b254-e6acf8f4276d",
   "metadata": {},
   "source": [
    "## Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1a4965-b224-472f-9675-4c17f83f548f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNN(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        n_features: int,\n",
    "        layer_sizes: list[int],\n",
    "        dropout: float,\n",
    "        learning_rate: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        layer_sizes = (n_features,) + tuple(layer_sizes)\n",
    "        layers = []\n",
    "        for chan_in, chan_out in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            layers.extend(\n",
    "                [\n",
    "                    nn.Conv2d(chan_in, chan_out, kernel_size=3, stride=1, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(p=dropout),\n",
    "                ]\n",
    "            )\n",
    "        layers.append(\n",
    "            nn.Conv2d(layer_sizes[-1], n_features, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "        self.cnn = nn.Sequential(*layers)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_function = F.l1_loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = self.loss_function(outputs, targets)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = self.loss_function(outputs, targets)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return {\"optimizer\": optimizer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9493902-8c1e-40bd-b186-621b56aca9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = data_preparation_normed[train_start][0].shape[-3]\n",
    "model_params = dict(\n",
    "    n_features=n_features, layer_sizes=[64, 64], dropout=0.6, learning_rate=1e-5\n",
    ")\n",
    "model = CNN(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fa9f5a-b2b1-4952-a066-831b66d28917",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aa6230-b381-4c04-b231-9cf432148079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensures that we use CPU if a GPU is available, uncomment to use your GPU\n",
    "%env CUDA_VISIBLE_DEVICES="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfa3d6f-4f15-4e69-af25-246a811ef81b",
   "metadata": {},
   "source": [
    "**Note:** Here we use `forkserver` to prevent deadlocks on Linux platform when using more than one worker in the data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615389bf-2e4d-4cac-abe0-06637d699792",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = edit.training.data.lightning.PipelineLightningDataModule(\n",
    "    data_preparation_normed,\n",
    "    train_split=train_split,\n",
    "    valid_split=val_split,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=n_workers,\n",
    "    multiprocessing_context=\"forkserver\",\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54e16c5-dfe4-4c4b-bea7-ab7646591b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b41f4f-a13d-45e2-bc25-5b8b6bd6555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpt_path = Path(default_root_dir) / \"model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3615a824-48cd-4dd3-8da4-3f0cce4d0c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = edit.training.lightning.Train(\n",
    "    model,\n",
    "    data_module,\n",
    "    default_root_dir,\n",
    "    max_epochs=max_epochs,\n",
    "    callbacks=[RichProgressBar()]\n",
    ")\n",
    "trainer.fit(load=False)\n",
    "# trainer.save(chkpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba3ee31-f7e2-4a9d-ac52-0dd44a60a871",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744a297f-940f-46c7-abdc-d4fe344a2718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CNN.load_from_checkpoint(chkpt_path, **model_params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c43370d-7d57-4636-a0cb-2734a665b7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a pipeline used to convert back prediction in original space\n",
    "undo_pipeline = edit.pipeline.Pipeline(*data_preparation_normed.steps[-5:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781e0f45-866c-4473-86f1-83c5d992ffdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "undo_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e02440-b514-44ad-bd8e-5846f34a564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_split = edit.pipeline.iterators.DateRange(test_start, test_end, interval=\"1h\")\n",
    "\n",
    "y_true = []\n",
    "y_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_date in test_split:\n",
    "        X, y = data_preparation_normed[test_date]\n",
    "        y_pred = model(torch.from_numpy(X)).cpu().numpy()\n",
    "\n",
    "        y = undo_pipeline.undo(y)\n",
    "        y[\"time\"] = y.time.copy(data=[test_date.datetime64()])\n",
    "        y_true.append(y)\n",
    "\n",
    "        y_pred = undo_pipeline.undo(y_pred)\n",
    "        y_pred[\"time\"] = y_pred.time.copy(data=[test_date.datetime64()])\n",
    "        y_preds.append(y_pred)\n",
    "\n",
    "y_true = xr.concat(y_true, dim=\"time\")\n",
    "y_preds = xr.concat(y_preds, dim=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1aa0c0-e5af-4aa1-9363-244e78b4a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd145ba-4761-4d41-aa14-7dbf8042bb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = y_preds.isel(time=slice(3))[\"z850\"].plot(col=\"time\")\n",
    "grid.fig.suptitle(\"Predictions\", y=1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee675ea9-62a0-4c46-acbb-a99f71f9113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = y_true.isel(time=slice(3))[\"z850\"].plot(col=\"time\")\n",
    "grid.fig.suptitle(\"Ground truth\", y=1.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9bda5c-2444-4f22-9aa2-053c1c568bdc",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c923d43-c76b-478c-a6c2-280aa7cb50df",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_score = scores.continuous.mae(y_preds, y_true, preserve_dims=[\"latitude\", \"longitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0697141f-d144-47e5-922a-5f292b6a5bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e43f0ff-97f2-4a86-9bc9-f419dce1c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_score[[\"z500\", \"z850\", \"z1000\"]].to_array(dim=\"field\").plot(col=\"field\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2733a1-0628-4399-9c74-3251ca396998",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EDIT-tutorial",
   "language": "python",
   "name": "edit-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
